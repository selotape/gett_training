{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T19:44:22.919640Z",
     "start_time": "2017-07-30T19:44:22.916634Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"big\">\n",
    "<font size=\"4\">\n",
    "in this section we download article pages from the popular news site Ynet. <br><br>\n",
    "We use BeautifulSoup to parse the html structure  <br><br>\n",
    "Try it Yourself Â»\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T19:50:35.536236Z",
     "start_time": "2017-07-30T19:50:35.529216Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract text and images features from an article\n",
    "def site_article_feature_extract(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content,\"lxml\")\n",
    "    \n",
    "    # kill all script elements\n",
    "    for script in soup(\"script\"):\n",
    "        script.extract()\n",
    "        \n",
    "    #extract features from page\n",
    "    page_content = soup.find_all('div', class_='block B3')[0]\n",
    "    features = {}\n",
    "    features['headline'] = page_content.find('div', class_='art_header_title').get_text()\n",
    "    features['sub_headline'] = page_content.find('div', class_='art_header_sub_title').get_text()\n",
    "    features['article_text'] = \" \".join([text.get_text().strip() for text in page_content.find_all('p')])\n",
    "    features['images'] = [x['src'] for x in page_content.find_all('img') if 'http://images' in x['src']]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T19:38:13.907913Z",
     "start_time": "2017-07-30T19:38:13.904910Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#archive articles from Jan 2017\n",
    "site_url = 'http://www.ynet.co.il/home/0,7340,L-4269-141-344-201701-1,00.html'\n",
    "page =  requests.get(site_url)\n",
    "soup = BeautifulSoup(page.content,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T19:43:22.484222Z",
     "start_time": "2017-07-30T19:43:22.121711Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "archive_page = soup.find_all('a', class_='smallheader') # articles in this site archive are called \"smallheader\"...\n",
    "article_urls = [urllib.parse.urljoin(\"http://www.ynet.co.il\", article.get('href')) for article in archive_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:02:20.352813Z",
     "start_time": "2017-07-30T19:59:39.612702Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scrapping sites one by one and extracting features\n",
    "df_articles = pd.DataFrame(columns=['headline', 'sub_headline', 'article_text', 'images'])\n",
    "i=0\n",
    "for url in article_urls:\n",
    "    i+=1\n",
    "    print('\\r extracting: {} , {} out of {}'.format(url, i, len(article_urls))),\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        df_articles.loc[len(df_articles)] = site_article_feature_extract(url)\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:02:34.364514Z",
     "start_time": "2017-07-30T20:02:34.356493Z"
    }
   },
   "outputs": [],
   "source": [
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:03:10.118937Z",
     "start_time": "2017-07-30T20:03:10.104901Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_articles.to_csv('./df_articles.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:03:24.552123Z",
     "start_time": "2017-07-30T20:03:24.540065Z"
    }
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading raw data we need to start preprocessing it <br>\n",
    "When it comes to text analysis we usually \"clean\" the text from any punctuations, white spaces and other symbols, leaving only words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:17:25.384422Z",
     "start_time": "2017-07-30T20:17:25.381416Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:05:09.945995Z",
     "start_time": "2017-07-30T20:05:09.933961Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_articles = pd.read_csv('./df_articles.csv' ,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:05:10.871204Z",
     "start_time": "2017-07-30T20:05:10.867193Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concat all text fields into one\n",
    "df_articles['all_text'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:05:11.755071Z",
     "start_time": "2017-07-30T20:05:11.731025Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean the text, leave only alpha, num and space\n",
    "pattern = re.compile(u\"[?????]\")\n",
    "df_articles['all_text'] = df_articles.apply(lambda row: re.sub(pattern, \"\", row['all_text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:05:50.201662Z",
     "start_time": "2017-07-30T20:05:50.141418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "#train this count vectorizer to give u the frequency of each word in our dataset\n",
    "#save your results into a pd.DataFrame that contains 2 columns: [word, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:06:07.715189Z",
     "start_time": "2017-07-30T20:06:07.707137Z"
    }
   },
   "outputs": [],
   "source": [
    "df_words.sort_values(by='count', ascending=[False]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:06:48.701734Z",
     "start_time": "2017-07-30T20:06:48.518246Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot a histogram of the words frequencies \n",
    "df_words['count'].plot(kind=)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will fit our data with <i>tf-idf</i> transformation. <br>\n",
    "Each document will be represented in a V dimension <br>\n",
    "The numbers on each feature vector should represent the tf-idf score of each word in V for this sample (document) <br>\n",
    "After extracting the <i>tf-idf</i> features, we can then use a dimensionality reduction technique called <b>PCA</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(min_df=?, max_df=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:17:52.024777Z",
     "start_time": "2017-07-30T20:17:51.897439Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf_vectors = \n",
    "svd = PCA(n_components=?, random_state=2017)\n",
    "tf_idf_vectors = svd.fit_transform(tf_idf_vectors)\n",
    "df_vectors = pd.DataFrame(tf_idf_vectors, columns=[\"tf_idf_\" +str(x) for x in range(tf_idf_vectors.shape[1])])\n",
    "df_vectors['headline'] = df_articles['headline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> Finally!! </font> <br> <br>\n",
    "After cleaning the text and extracting features we can go ahead and use K-Means to find clusters in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:17:54.692339Z",
     "start_time": "2017-07-30T20:17:54.689331Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:17:54.981083Z",
     "start_time": "2017-07-30T20:17:54.978075Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = df_vectors.columns.tolist()\n",
    "features.remove('headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:21:34.051182Z",
     "start_time": "2017-07-30T20:21:34.047199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters = ?\n",
    "kmeans = KMeans(?, random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:21:34.301228Z",
     "start_time": "2017-07-30T20:21:34.282177Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit kmeans to our data, use predict method to get the cluster for each sample\n",
    "kmeans.fit_transform(?)\n",
    "clusters = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:21:34.422316Z",
     "start_time": "2017-07-30T20:21:34.419308Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the assigned cluster to the corresponding row of the original dataset\n",
    "df_clusters = pd.DataFrame(columns=['headline', 'cluster'])\n",
    "df_clusters['headline'] = df_vectors['headline']\n",
    "df_clusters['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets see what we got\n",
    "df_clusters[df_clusters['cluster']==1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
